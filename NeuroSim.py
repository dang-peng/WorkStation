import numpy as np

# åŠ è½½MNISTæ•°æ®é›†
from MNIST import (x_train, y_train, x_test, y_test)


# å®šä¹‰ç¥ç»ç½‘ç»œæ¨¡å‹ç±»
class NeuralNetwork:
    def __init__(self, layer_sizes, lr):
        self.learning_rate = lr
        self.layer_input = None
        self.activations = None
        self.layer_sizes = layer_sizes
        self.num_layers = len(layer_sizes)
        self.weights = [np.random.randn(layer_sizes[i], layer_sizes[i - 1]) for i in range(1, self.num_layers)]
        self.biases = [np.zeros((layer_sizes[i], 1)) for i in range(1, self.num_layers)]
    
    # æ¿€æ´»å‡½æ•°
    @staticmethod
    def sigmoid(x):
        return 1 / (1 + np.exp(-x))
    
    # Sigmoidçš„å¯¼æ•°
    def sigmoid_derivative(self, z):
        return self.sigmoid(z) * (1 - self.sigmoid(z))
    
    @staticmethod
    def loss_derivative(activations, y):
        # âˆ‚L/âˆ‚y = y-Y
        result = [(a - y) for a, y in zip(activations, y)]
        return result
    
    @staticmethod
    def calculate_loss(x, y):
        return 0.5 * (x - y) ** 2
    
    # å‰å‘ä¼ æ’­
    def forward(self, x):
        activation = x
        self.activations = [x]
        self.layer_input = []  # å­˜å‚¨é™¤ç¬¬ä¸€å±‚ä¹‹å¤–çš„æ¯ä¸€å±‚è¾“å…¥
        for i in range(self.num_layers - 1):
            z = np.dot(self.weights[i], self.activations[i]) + self.biases[i]
            self.layer_input.append(z)
            activation = self.sigmoid(z)
            self.activations.append(activation)
        return self.activations[-1]
    
    # åå‘ä¼ æ’­å’Œå‚æ•°æ›´æ–°
    def backward(self, x, y):
        m = x.shape[0]
        # ğ›¿ = (y-Y)*y*(1-y)
        # error = self.loss_derivative(self.activations[-1], y)
        deltas = [self.loss_derivative(self.activations[-1], y) * self.sigmoid_derivative(self.layer_input[-1])]
        # delta = error * self.sigmoid_derivative(self.layer_input[-1])
        for i in range(self.num_layers - 2, 0, -1):
            delta = np.dot(self.weights[i].T, deltas[-1]) * (self.activations[i] * (1 - self.activations[i]))
            deltas.append(delta)
        deltas.reverse()
        # âˆ‚Lâ±¼/âˆ‚wáµ¢ = ğ›¿*X
        dL_dW = [np.dot(deltas[i], self.activations[i].T) / m for i in range(self.num_layers - 1)]
        dL_db = [np.mean(deltas[i], axis = 1, keepdims = True) for i in range(self.num_layers - 1)]
        # å‚æ•°æ›´æ–°
        self.weights = [self.weights[i] - self.learning_rate * dL_dW[i] for i in range(self.num_layers - 1)]
        self.biases = [self.biases[i] - self.learning_rate * dL_db[i] for i in range(self.num_layers - 1)]
    
    # äº¤å‰ç†µæŸå¤±å‡½æ•°
    def compute_loss(self, x, y):
        m = x.shape[0]  # æ ·æœ¬æ•°é‡
        log_probs = -np.log(self.forward(x)[y, np.arange(m)])  # è®¡ç®—æ¯ä¸ªæ ·æœ¬å¯¹åº”ç±»åˆ«çš„è´Ÿå¯¹æ•°æ¦‚ç‡
        loss = np.sum(log_probs) / m  # è®¡ç®—å¹³å‡æŸå¤±
        return loss * 10 ** 8
    
    # è®¡ç®—å‡æ–¹è¯¯å·®æŸå¤±å‡½æ•°
    def mean_squared_error(self, x, y):
        m = x.shape[0]  # æ ·æœ¬æ•°é‡
        test_result = self.forward(x)[y, np.arange(m)]
        return np.sum([0.5 * (x - y) ** 2 for (x, y) in test_result])
    
    # æ¨¡å‹è®­ç»ƒ
    def train(self, x_train, y_train):
        for i in range(len(x_train)):
            self.forward(x_train[i])
            self.backward(x_train[i], y_train[i])
            # è®¡ç®—å¹¶è¾“å‡ºæŸå¤±
            loss = self.compute_loss(x_train[i], y_train[i])
            print("Epoch {} Loss: {:.4f}".format(epoch + 1, loss))
        # mse = self.mean_squared_error(x_train, y_train)
        # print("Epoch {} Loss: {:.4f}".format(epoch + 1, mse))
    
    # é¢„æµ‹
    def predict(self, test_input):
        test_result = [self.forward(x) for x in test_input]
        return test_result
    
    def accuracy(self, x_test, y_test):
        count = 0
        for i in range(len(x_test)):
            y_hat = self.forward(x_test[i])
            y_pred = np.argmax(y_hat, axis = 0)
            for j in range(len(x_test[i])):
                if y_pred[i] == y_test[i][j]:
                    count = count + 1
        sum_samples = len(x_test) * len(x_test[0])
        accuracy = 100 * count / sum_samples
        return accuracy


# è®¾ç½®è¶…å‚æ•°
learning_rate = 0.01
num_epochs = 10
# åˆ›å»ºç¥ç»ç½‘ç»œæ¨¡å‹
input_size = 100
hidden_sizes = 64  # å¯æ ¹æ®éœ€æ±‚è®¾ç½®éšè—å±‚çš„å¤§å°å’Œå±‚æ•°
output_size = 10
layer_sizes = [input_size] + [hidden_sizes] + [output_size]
model = NeuralNetwork(layer_sizes, learning_rate)
# è®­ç»ƒæ¨¡å‹
# from MNIST import (x_train, y_train, x_test, y_test)
for epoch in range(1000):
    model.train(x_train, y_train)
accuracy = model.accuracy(x_test, y_test)
print("Inference Accuracy: {:.2f}".format(accuracy))
